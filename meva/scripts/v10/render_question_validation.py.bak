#!/usr/bin/env python3
"""
Render question validation videos: multi-camera grid with geom overlays.

Creates a video showing all cameras in a question side-by-side/grid, with:
- Bounding boxes from geom.yml overlayed (scaled to display resolution)
- Time span limited to the answer frame range
- Camera labels and timestamps
- Question text overlay at the bottom

Handles ALL question categories:
  temporal      → event_a / event_b dicts
  event_ordering→ debug_info["events"] list
  perception    → representative_event dict
  spatial       → entity_a / entity_b dicts
  summarization → clip_files list (no events)
  counting      → clip_files or cameras_involved (no events)
  best_camera   → representative_event dict

Usage:
    python3 render_question_validation.py --slot 2018-03-07.17-05.school --question-id 0
    python3 render_question_validation.py --slot 2018-03-07.17-05.school --all
    python3 render_question_validation.py --qa-file qa.json --question-id 5 -v
"""

import argparse
import json
import re
import cv2
import numpy as np
from pathlib import Path
from collections import defaultdict
import subprocess
import shutil
from typing import Dict, List, Optional, Tuple


def _find_ffmpeg() -> Optional[str]:
    """Find ffmpeg binary — system PATH first, then imageio-ffmpeg fallback."""
    path = shutil.which("ffmpeg")
    if path:
        return path
    try:
        import imageio_ffmpeg
        return imageio_ffmpeg.get_ffmpeg_exe()
    except ImportError:
        return None

FFMPEG_BIN = _find_ffmpeg()

# ============================================================================
# Paths
# ============================================================================
MEVA_MP4_ROOT = Path("/nas/mars/dataset/MEVA/mp4s")
KITWARE_BASE = Path("/nas/mars/dataset/MEVA/meva-data-repo/annotation/DIVA-phase-2/MEVA/kitware")
KITWARE_TRAINING_BASE = Path("/nas/mars/dataset/MEVA/meva-data-repo/annotation/DIVA-phase-2/MEVA/kitware-meva-training")
QA_OUTPUT_DIR = Path("/home/ah66742/data/qa_pairs")
VIDEO_OUTPUT_DIR = Path("/home/ah66742/output/validation_videos")

VIDEO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================================
# Display constants
# ============================================================================
CELL_W, CELL_H = 640, 360          # Each camera cell
PAD = 20                            # Padding between cells
HEADER_H = 60                       # Question text header height
COLOR_BOX   = (0, 255, 0)           # Green bounding boxes
COLOR_TEXT  = (255, 255, 255)        # White labels
COLOR_BGND  = (40, 40, 40)          # Dark gray canvas
COLOR_CAM   = (0, 200, 255)         # Orange camera label
MAX_RENDER_FRAMES = 300             # Cap at 10s @ 30fps (avoid huge files)
DEFAULT_CONTEXT_SEC = 5.0           # For categories with no frame range

# ============================================================================
# Geom parsing (regex approach — fast, no YAML dependency)
# ============================================================================
_RE_ID1 = re.compile(r"['\"]?id1['\"]?\s*:\s*['\"]?(\d+)")
_RE_TS0 = re.compile(r"['\"]?ts0['\"]?\s*:\s*['\"]?(\d+)")
_RE_G0  = re.compile(r"['\"]?g0['\"]?\s*:\s*['\"]?(\d+)\s+(\d+)\s+(\d+)\s+(\d+)")


def parse_geom_boxes(geom_file: Path) -> Dict[int, list]:
    """Extract bounding boxes from geom.yml by frame number.

    Returns {frame_number: [(x1, y1, x2, y2, actor_id), ...]}
    """
    boxes_by_frame: Dict[int, list] = defaultdict(list)
    if not geom_file.exists():
        return boxes_by_frame

    with open(geom_file) as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            m_id = _RE_ID1.search(line)
            m_ts = _RE_TS0.search(line)
            m_g0 = _RE_G0.search(line)
            if m_id and m_ts and m_g0:
                actor_id = int(m_id.group(1))
                frame = int(m_ts.group(1))
                x1, y1, x2, y2 = map(int, m_g0.groups())
                boxes_by_frame[frame].append((x1, y1, x2, y2, actor_id))

    return dict(boxes_by_frame)


# ============================================================================
# File finders
# ============================================================================

def _slot_dir_name(date: str, start_time: str, site: str) -> str:
    """Build slot directory name using HH-MM format (no seconds).
    
    Clip filenames use HH-MM-SS, but slot directories use HH-MM.
    Example: start_time='17-05-00' → dir '2018-03-07.17-05.school'
    """
    parts = start_time.split("-")
    hh_mm = f"{parts[0]}-{parts[1]}" if len(parts) >= 2 else start_time
    return f"{date}.{hh_mm}.{site}"


def find_mp4(date: str, hour: str, start_time: str, site: str,
             end_time: str, camera: str) -> Optional[Path]:
    """Find MP4 file for a camera, using slot-grouped directory structure.
    
    Directory: /mp4s/{date}/{hour}/{date}.{HH-MM}.{site}/
    File:      {date}.{start_time}.{end_time}.{site}.{camera}.r13.mp4
    """
    slot_name = _slot_dir_name(date, start_time, site)
    slot_dir = MEVA_MP4_ROOT / date / hour / slot_name
    if not slot_dir.exists():
        # Fallback: try globbing for the slot directory
        parent = MEVA_MP4_ROOT / date / hour
        if parent.exists():
            candidates = list(parent.glob(f"{date}.*{site}"))
            if candidates:
                slot_dir = candidates[0]
            else:
                return None
        else:
            return None

    # Try exact glob
    pattern = f"*{camera}*.r13.mp4"
    matches = list(slot_dir.glob(pattern))
    if matches:
        return matches[0]
    # Broader fallback
    matches = list(slot_dir.glob(f"*{camera}*.mp4"))
    return matches[0] if matches else None


def find_geom_file(date: str, hour: str, start_time: str,
                   site: str, camera: str) -> Optional[Path]:
    """Find geom.yml file for a camera."""
    prefix = f"{date}.{start_time}"
    for kitware_dir in [KITWARE_BASE, KITWARE_TRAINING_BASE]:
        ann_dir = kitware_dir / date / hour
        if not ann_dir.exists():
            continue
        matches = list(ann_dir.glob(f"{prefix}*.{site}.{camera}.geom.yml"))
        if matches:
            return matches[0]
    return None


def extract_clip_timing(clip_file: str) -> Optional[Tuple[str, str, str, str, str, str]]:
    """Parse clip filename → (date, hour, start_time, end_time, site, camera).

    Input: '2018-03-07.17-05-00.17-10-00.school.G339.r13.mp4'
    """
    name = Path(clip_file).name
    # Strip common suffixes
    for suffix in [".r13.mp4", ".r13.avi", ".mp4", ".avi"]:
        if name.endswith(suffix):
            name = name[: -len(suffix)]
            break
    parts = name.split(".")
    if len(parts) >= 5:
        date, start_time, end_time, site, camera = parts[0], parts[1], parts[2], parts[3], parts[4]
        hour = start_time.split("-")[0]
        return date, hour, start_time, end_time, site, camera
    return None


# ============================================================================
# Event extraction — handles ALL debug_info formats
# ============================================================================

def _extract_events(q: dict, slot: str) -> List[dict]:
    """Extract a unified list of event dicts from any question category.

    Each returned event dict has at minimum:
        camera, clip_file, frame_range, fps, label
    """
    debug = q.get("debug_info", {})
    category = q.get("category", "")
    events: List[dict] = []

    # --- Pattern 1: top-level event dicts (temporal, perception, spatial, best_camera) ---
    for key, val in debug.items():
        if isinstance(val, dict) and "camera" in val and val.get("clip_file"):
            events.append({
                "camera":      val["camera"],
                "clip_file":   val["clip_file"],
                "frame_range": val.get("frame_range"),
                "fps":         val.get("fps", 30.0),
                "actor_ids":   val.get("actor_ids", []),
                "label":       val.get("activity", key),
            })

    # --- Pattern 2: events LIST (event_ordering) ---
    if "events" in debug and isinstance(debug["events"], list):
        for ev in debug["events"]:
            if isinstance(ev, dict) and "camera" in ev and ev.get("clip_file"):
                events.append({
                    "camera":      ev["camera"],
                    "clip_file":   ev["clip_file"],
                    "frame_range": ev.get("frame_range"),
                    "fps":         ev.get("fps", 30.0),
                    "actor_ids":   ev.get("actor_ids", []),
                    "label":       ev.get("activity", "event"),
                })

    # --- Pattern 3: no events, but clip_files list (summarization, counting) ---
    if not events and "clip_files" in debug:
        for cf in debug["clip_files"]:
            timing = extract_clip_timing(cf)
            if timing:
                _date, _hour, _st, _et, _site, _camera = timing
                events.append({
                    "camera":      _camera,
                    "clip_file":   cf,
                    "frame_range": None,
                    "fps":         30.0,
                    "actor_ids":   [],
                    "label":       category,
                })

    # --- Pattern 4: no events, no clip_files, but cameras list ---
    if not events:
        cameras = q.get("requires_cameras", [])
        # Try to derive clip_files from slot name
        # slot format: 2018-03-07.17-05.school
        slot_parts = slot.split(".")
        if len(slot_parts) >= 3:
            date = slot_parts[0]
            hhmm = slot_parts[1]  # e.g. 17-05
            site = slot_parts[2]
            hour = hhmm.split("-")[0]
            for cam in cameras:
                mp4 = find_mp4(date, hour, f"{hhmm}-00", site, "", cam)
                if mp4:
                    events.append({
                        "camera":      cam,
                        "clip_file":   mp4.name,
                        "frame_range": None,
                        "fps":         30.0,
                        "actor_ids":   [],
                        "label":       category,
                    })

    return events


# ============================================================================
# Frame loading with proper scaling
# ============================================================================

def load_video_segment(mp4_path: Path,
                       frame_start: int,
                       frame_end: int,
                       target_w: int = CELL_W,
                       target_h: int = CELL_H
                       ) -> Tuple[Optional[List[np.ndarray]], float, float, float]:
    """Load a range of frames, resized to target dims.

    Returns (frames, fps, scale_x, scale_y) or (None, 0, 0, 0).
    scale_x/y map original coords → display coords for box scaling.
    """
    if not mp4_path or not mp4_path.exists():
        return None, 0, 0, 0

    cap = cv2.VideoCapture(str(mp4_path))
    if not cap.isOpened():
        return None, 0, 0, 0

    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    orig_w = cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 1920
    orig_h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 1072
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Clamp frame range
    frame_start = max(0, frame_start)
    frame_end = min(frame_end, total_frames - 1)
    n_frames = min(frame_end - frame_start + 1, MAX_RENDER_FRAMES)

    scale_x = target_w / orig_w
    scale_y = target_h / orig_h

    frames = []
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start)
    for _ in range(n_frames):
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (target_w, target_h)))

    cap.release()
    return (frames, fps, scale_x, scale_y) if frames else (None, 0, 0, 0)


def overlay_boxes(frame: np.ndarray, boxes: list,
                  scale_x: float, scale_y: float) -> np.ndarray:
    """Draw scaled bounding boxes on a (resized) frame."""
    out = frame.copy()
    for (x1, y1, x2, y2, actor_id) in boxes:
        sx1, sy1 = int(x1 * scale_x), int(y1 * scale_y)
        sx2, sy2 = int(x2 * scale_x), int(y2 * scale_y)
        cv2.rectangle(out, (sx1, sy1), (sx2, sy2), COLOR_BOX, 2)
        label = f"A{actor_id}"
        cv2.putText(out, label, (sx1, max(sy1 - 5, 12)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR_TEXT, 1, cv2.LINE_AA)
    return out


# ============================================================================
# Grid composition
# ============================================================================

def compose_grid(frames_dict: Dict[str, List[np.ndarray]],
                 question_text: str,
                 answer_text: str = "",
                 fps: float = 30.0) -> List[np.ndarray]:
    """Compose multi-camera views into a labelled grid with question header.

    Uses cameras present in frames_dict (not requires_cameras) so only
    cameras that were successfully loaded appear.
    """
    cameras = sorted(frames_dict.keys())
    n_cams = len(cameras)
    if n_cams == 0:
        return []

    # Grid layout — compute rows/cols dynamically
    import math
    if n_cams == 1:
        grid_rows, grid_cols = 1, 1
    elif n_cams == 2:
        grid_rows, grid_cols = 1, 2
    elif n_cams <= 4:
        grid_rows, grid_cols = 2, 2
    elif n_cams <= 6:
        grid_rows, grid_cols = 2, 3
    else:
        grid_cols = min(4, math.ceil(math.sqrt(n_cams)))
        grid_rows = math.ceil(n_cams / grid_cols)

    comp_w = grid_cols * CELL_W + PAD * (grid_cols + 1)
    comp_h = HEADER_H + grid_rows * CELL_H + PAD * (grid_rows + 1)

    # Pad shorter camera feeds with last-frame freeze
    max_frames = max(len(v) for v in frames_dict.values())
    for cam in cameras:
        deficit = max_frames - len(frames_dict[cam])
        if deficit > 0:
            frames_dict[cam].extend([frames_dict[cam][-1]] * deficit)

    composed = []
    for fi in range(max_frames):
        canvas = np.full((comp_h, comp_w, 3), COLOR_BGND, dtype=np.uint8)

        # Draw question text header
        q_short = question_text[:120] + ("..." if len(question_text) > 120 else "")
        cv2.putText(canvas, q_short, (PAD, 35),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_TEXT, 1, cv2.LINE_AA)
        if answer_text:
            a_short = f"Answer: {answer_text[:80]}"
            cv2.putText(canvas, a_short, (PAD, 52),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 100), 1, cv2.LINE_AA)

        for ci, cam in enumerate(cameras):
            row, col = ci // grid_cols, ci % grid_cols
            x = PAD + col * (CELL_W + PAD)
            y = HEADER_H + PAD + row * (CELL_H + PAD)

            # Safety check — skip if cell falls outside canvas
            if y + CELL_H > comp_h or x + CELL_W > comp_w:
                continue

            frame = frames_dict[cam][fi]
            canvas[y:y + CELL_H, x:x + CELL_W] = frame

            # Camera label with frame counter
            label = f"{cam}  [{fi + 1}/{max_frames}]"
            cv2.putText(canvas, label, (x + 8, y + 22),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, COLOR_CAM, 2, cv2.LINE_AA)

        composed.append(canvas)

    return composed


# ============================================================================
# Video writer
# ============================================================================

def write_video(frames: List[np.ndarray], output_path: Path, fps: float = 30.0) -> bool:
    """Write frames to MP4 — uses ffmpeg if available, cv2.VideoWriter fallback."""
    if not frames:
        return False

    h, w = frames[0].shape[:2]
    # Ensure dimensions are even (required by libx264)
    if w % 2: w -= 1
    if h % 2: h -= 1

    output_path.parent.mkdir(parents=True, exist_ok=True)

    if FFMPEG_BIN:
        cmd = [
            FFMPEG_BIN, '-hide_banner', '-loglevel', 'error',
            '-f', 'rawvideo', '-pix_fmt', 'bgr24',
            '-s', f'{w}x{h}', '-r', str(fps),
            '-i', '-',
            '-c:v', 'libx264', '-crf', '26', '-preset', 'fast',
            '-pix_fmt', 'yuv420p', '-y', str(output_path),
        ]
        proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stderr=subprocess.PIPE)
        for frame in frames:
            proc.stdin.write(frame[:h, :w].tobytes())
        proc.stdin.close()
        proc.wait()
        if proc.returncode != 0:
            stderr = proc.stderr.read().decode()
            print(f"  ffmpeg error: {stderr[:200]}")
            return False
        return output_path.exists()
    else:
        # Fallback: cv2.VideoWriter with mp4v codec
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(str(output_path), fourcc, fps, (w, h))
        if not writer.isOpened():
            print(f"  cv2.VideoWriter failed to open")
            return False
        for frame in frames:
            writer.write(frame[:h, :w])
        writer.release()
        return output_path.exists()


# ============================================================================
# Main render function
# ============================================================================

def render_question(qa_data: dict, question_idx: int,
                    output_path: Path = None, verbose: bool = False) -> Optional[Path]:
    """Render a single question into a validation video.

    Works for ALL question categories by using _extract_events().
    """
    qa_pairs = qa_data.get("qa_pairs", [])
    if question_idx >= len(qa_pairs):
        print(f"  ERROR: Q{question_idx} out of range (max {len(qa_pairs) - 1})")
        return None

    q = qa_pairs[question_idx]
    slot = qa_data.get("slot", "unknown")
    category = q.get("category", "unknown")
    q_text = q.get("naturalized_question") or q.get("question_template", "")
    answer_text = q.get("correct_answer") or q.get("answer", "")

    print(f"\n{'='*60}")
    print(f"Q{question_idx}: {category.upper()}")
    print(f"  {q_text[:100]}...")
    print(f"  Answer: {answer_text[:80]}")

    # Extract events from debug_info (handles all category formats)
    events = _extract_events(q, slot)
    if not events:
        print(f"  SKIP: No video events found in debug_info")
        return None

    # Group events by camera
    cam_events: Dict[str, list] = defaultdict(list)
    for ev in events:
        cam_events[ev["camera"]].append(ev)

    # Compute frame range per camera (union of all events on that camera)
    cam_info: Dict[str, dict] = {}
    for cam, evts in cam_events.items():
        # Pick clip_file from first event (all events on same camera share clip)
        clip_file = evts[0]["clip_file"]
        fps = evts[0]["fps"]
        timing = extract_clip_timing(clip_file)
        if not timing:
            print(f"  WARN: Cannot parse clip_file for {cam}: {clip_file}")
            continue

        date, hour, start_time, end_time, site, _ = timing

        # Compute frame range — union of all events, with context padding
        ranges = [e["frame_range"] for e in evts if e.get("frame_range")]
        if ranges:
            f_start = max(0, min(r[0] for r in ranges) - int(fps * 1.0))  # 1s before
            f_end   = max(r[1] for r in ranges) + int(fps * 1.0)            # 1s after
        else:
            # No frame range (summarization/counting) — show middle 5 seconds
            f_start = 0
            f_end = int(fps * DEFAULT_CONTEXT_SEC)

        cam_info[cam] = {
            "clip_file":  clip_file,
            "fps":        fps,
            "date":       date,
            "hour":       hour,
            "start_time": start_time,
            "end_time":   end_time,
            "site":       site,
            "f_start":    f_start,
            "f_end":      f_end,
            "actor_ids":  list({aid for e in evts for aid in e.get("actor_ids", [])}),
        }

    if not cam_info:
        print(f"  SKIP: No parseable camera info")
        return None

    # Load frames + geom for each camera
    frames_dict: Dict[str, List[np.ndarray]] = {}
    scales: Dict[str, Tuple[float, float]] = {}
    geom_dict: Dict[str, Dict[int, list]] = {}

    for cam, info in cam_info.items():
        mp4_path = find_mp4(info["date"], info["hour"], info["start_time"],
                            info["site"], info["end_time"], cam)
        if not mp4_path:
            if verbose:
                print(f"  {cam}: MP4 NOT FOUND")
            continue

        frames, fps, sx, sy = load_video_segment(
            mp4_path, info["f_start"], info["f_end"])
        if not frames:
            if verbose:
                print(f"  {cam}: FAILED TO LOAD")
            continue

        frames_dict[cam] = frames
        scales[cam] = (sx, sy)

        # Load geom
        geom_path = find_geom_file(info["date"], info["hour"],
                                   info["start_time"], info["site"], cam)
        if geom_path:
            geom_dict[cam] = parse_geom_boxes(geom_path)

        n_geom = len(geom_dict.get(cam, {}))
        print(f"  {cam}: {len(frames)} frames "
              f"(f{info['f_start']}-{info['f_end']}) | "
              f"geom={n_geom} frames | {mp4_path.name}")

    if not frames_dict:
        print(f"  ERROR: No frames loaded for any camera")
        return None

    # Overlay geom bounding boxes (scaled)
    for cam in frames_dict:
        boxes_by_frame = geom_dict.get(cam, {})
        if not boxes_by_frame:
            continue
        sx, sy = scales[cam]
        f_offset = cam_info[cam]["f_start"]
        for fi in range(len(frames_dict[cam])):
            actual_frame = f_offset + fi
            if actual_frame in boxes_by_frame:
                frames_dict[cam][fi] = overlay_boxes(
                    frames_dict[cam][fi], boxes_by_frame[actual_frame], sx, sy)

    # Compose grid
    composed = compose_grid(frames_dict, q_text, answer_text, 30.0)
    if not composed:
        print(f"  ERROR: Grid composition produced no frames")
        return None

    # Output path
    if not output_path:
        output_path = VIDEO_OUTPUT_DIR / f"{slot}_q{question_idx}_{category}.mp4"

    print(f"  Writing {len(composed)} frames → {output_path.name} ...", end="", flush=True)
    if write_video(composed, output_path, 30.0):
        sz_mb = output_path.stat().st_size / (1024 * 1024)
        print(f"  OK ({sz_mb:.1f} MB)")
        return output_path
    else:
        print(f"  FAILED")
        return None


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Render question validation videos with multi-camera grids")
    parser.add_argument("--slot", help="Slot name (e.g., 2018-03-07.17-05.school)")
    parser.add_argument("--qa-file", help="QA JSON file path")
    parser.add_argument("--question-id", type=int, help="Question index to render (0-based)")
    parser.add_argument("--all", action="store_true", help="Render ALL questions in the file")
    parser.add_argument("--categories", nargs="+",
                        help="Only render these categories (e.g., temporal spatial)")
    parser.add_argument("--output", "-o", help="Output video path (single question only)")
    parser.add_argument("--natural", action="store_true",
                        help="Use .final.naturalized.json instead of .final.raw.json")
    parser.add_argument("-v", "--verbose", action="store_true")
    args = parser.parse_args()

    # Resolve QA file
    if args.qa_file:
        qa_path = Path(args.qa_file)
    elif args.slot:
        suffix = ".final.naturalized.json" if args.natural else ".final.raw.json"
        qa_path = QA_OUTPUT_DIR / f"{args.slot}{suffix}"
        if not qa_path.exists() and args.natural:
            qa_path = QA_OUTPUT_DIR / f"{args.slot}.final.raw.json"
            print(f"  (naturalized not found, falling back to raw)")
    else:
        parser.error("Must provide --slot or --qa-file")

    if not qa_path.exists():
        print(f"ERROR: {qa_path} not found")
        return

    with open(qa_path) as f:
        qa_data = json.load(f)

    slot = qa_data.get("slot", "unknown")
    qa_pairs = qa_data.get("qa_pairs", [])
    print(f"Loaded: {qa_path.name}")
    print(f"Slot:   {slot}")
    print(f"Questions: {len(qa_pairs)}")

    # Determine which questions to render
    if args.all:
        indices = list(range(len(qa_pairs)))
    elif args.question_id is not None:
        indices = [args.question_id]
    else:
        indices = [0]

    # Filter by category if requested
    if args.categories:
        cats = set(c.lower() for c in args.categories)
        indices = [i for i in indices if qa_pairs[i].get("category", "").lower() in cats]

    print(f"Rendering: {len(indices)} question(s)")

    successes, failures = 0, 0
    for idx in indices:
        out = Path(args.output) if (args.output and len(indices) == 1) else None
        result = render_question(qa_data, idx, out, verbose=args.verbose)
        if result:
            successes += 1
        else:
            failures += 1

    print(f"\n{'='*60}")
    print(f"Done: {successes} rendered, {failures} skipped/failed")
    print(f"Output dir: {VIDEO_OUTPUT_DIR}")
    if successes:
        print(f"Play:  ffplay {VIDEO_OUTPUT_DIR}/{slot}_q*")


if __name__ == "__main__":
    main()
